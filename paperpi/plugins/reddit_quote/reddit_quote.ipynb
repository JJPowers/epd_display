{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function must import layout and constants\n",
    "# this is structured to work both in Jupyter notebook and from the command line\n",
    "try:\n",
    "    from . import layout\n",
    "    from . import constants\n",
    "except ImportError:\n",
    "    import layout\n",
    "    import constants\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import secrets\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "\n",
    "import requests\n",
    "from dictor import dictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_now():\n",
    "    return datetime.now().strftime(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_quotes():\n",
    "    '''fetch quotes from reddit'''\n",
    "    error = False\n",
    "    logging.debug('fetching data from reddit')\n",
    "    raw_quotes = [constants.error_text]\n",
    "    try:\n",
    "        r = requests.get(constants.quotes_url, headers=constants.headers)\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f'failed to fetch quotes from {constants.quotes_url}, {e}')\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            json_data = dictor(r.json(), constants.quote_data_addr)\n",
    "            raw_quotes = [dictor(q, constants.quote_title_addr) for q in json_data]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f'bad json data: {e}')\n",
    "            raw_quotes = [constants.error_text]\n",
    "            error = True\n",
    "    else:\n",
    "        logging.warning(f'error accessing {constants.quotes_url}: code {r.status_code}')\n",
    "        raw_quotes = [constants.error_text]\n",
    "        error = True\n",
    "        \n",
    "    if len(raw_quotes) < 1:\n",
    "        raw_quotes = [constants.error_text]\n",
    "        error = True\n",
    "        \n",
    "    return (raw_quotes, error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_quotes(raw_quotes):\n",
    "    processed_quotes = []\n",
    "    logging.debug(f'processing {len(raw_quotes)} quotes')\n",
    "    for quote in raw_quotes:\n",
    "        # make sure we have a string to work with\n",
    "        quote = str(quote)\n",
    "        # sub double quotes for any other quote character or '' \n",
    "        q = re.sub('“|”|\\'\\'|\"', '', quote)\n",
    "        # sub single quote for ’ character\n",
    "        q = re.sub('’', \"'\", q)\n",
    "        # sub minus for endash, emdash, hyphen, ~\n",
    "        q = re.sub('-|–|—|~|--|―', '-', q)\n",
    "        # clean trailing whitespace in quotes\n",
    "        q = re.sub('\\s+\"', '\"', q)\n",
    "        # split quote from attirbution\n",
    "        match = re.match('(.*)\\s{0,}-\\s{0,}(.*)', q)\n",
    "\n",
    "        if hasattr(match, 'groups'):\n",
    "            if len(match.groups()) > 1:\n",
    "                text = match.group(1).strip()\n",
    "                attribution = match.group(2).strip().title()\n",
    "            else:\n",
    "                text = match.group(1).strip()\n",
    "                attribution = None\n",
    "        else:\n",
    "            text = q.strip()\n",
    "            attribution = None\n",
    "\n",
    "        # append quotes to dictionary\n",
    "        \n",
    "        processed_quotes.append({'len': len(q), 'text': text, 'attribution': attribution})\n",
    "    return processed_quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure this function can accept *args and **kwargs even if you don't intend to use them\n",
    "def update_function(self, *args, **kwargs):\n",
    "    '''update function for reddit_quote plugin\n",
    "    \n",
    "    Scrapes quotes from reddit.com/r/quotes and displays them one at a time\n",
    "    \n",
    "   Requirements:\n",
    "        self.config(`dict`): {\n",
    "        'max_length': 144,   # name of player to track\n",
    "        'idle_timeout': 10,               # timeout for disabling plugin\n",
    "    }\n",
    "    self.cache(`CacheFiles` object)\n",
    "\n",
    "    Args:\n",
    "        self(namespace): namespace from plugin object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_updated(bool), data(dict), priority(int))        \n",
    "    \n",
    "    %U'''  \n",
    "\n",
    "    \n",
    "    logging.info(f'update function for {constants.name}')\n",
    "    json_file = self.cache.path/Path(constants.json_file)\n",
    "    \n",
    "    max_length = self.config.get('max_length', constants.required_config_options['max_length'])\n",
    "    max_retries = self.config.get('max_retries', constants.required_config_options['max_retries'])\n",
    "    \n",
    "    try:\n",
    "        max_length = int(max_length)\n",
    "        max_retries = int(max_retries)\n",
    "    except ValueError as e:\n",
    "        logging.warning('non-numeric values provided in configuration file for max_length or max_retries')\n",
    "    \n",
    "    is_updated = False\n",
    "    data = {}\n",
    "    priority = 2**16\n",
    "    \n",
    "    logging.debug(f'checking mtime of cached json file: {json_file}')\n",
    "    \n",
    "    # check the age of the cached data\n",
    "    try:\n",
    "        mtime  = time() - path.getmtime(json_file)\n",
    "        logging.debug(f'age of {json_file}: {mtime}')\n",
    "    except OSError as e:\n",
    "#         logging.info(f'{e}')\n",
    "        mtime = 2**16\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.info(f'{e}')\n",
    "        mtime = 2**16\n",
    "        \n",
    "    \n",
    "    if json_file.exists() and mtime < constants.json_max_age:\n",
    "        try:\n",
    "            logging.debug('using cached reddit data')\n",
    "            with open(json_file) as jf:\n",
    "                json_data = json.load(jf)\n",
    "        except OSError as e:\n",
    "            logging.warning(f'could not open cached JSON file: {e}')\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f'could not decode JSON file: {e}')\n",
    "            json_data = None\n",
    "    else:\n",
    "        logging.debug('cached data expired, fetching fresh data')\n",
    "        json_data = None\n",
    "            \n",
    "    \n",
    "    if not json_data:\n",
    "        logging.debug('downloading fresh data from reddit')\n",
    "        raw_quotes, fetch_error = _fetch_quotes()\n",
    "        json_data = _process_quotes(raw_quotes)\n",
    "        \n",
    "        if fetch_error:\n",
    "            logging.warning('failed to fetch data due to previous errors. skipping cache.')\n",
    "        else:\n",
    "            logging.info('caching data')\n",
    "            try:\n",
    "                with open(json_file, 'w')  as jf:\n",
    "                    json.dump(json_data, jf)\n",
    "            except OSError as e:\n",
    "                logging.error(f'failed to cache data: {e}')\n",
    "            \n",
    "    if json_data:  \n",
    "        for i in range(0, max_retries):\n",
    "            logging.debug(f'choosing quote with length < {max_length} characters')\n",
    "            my_quote = secrets.choice(json_data)\n",
    "            if my_quote['len'] < max_length:\n",
    "                break\n",
    "            else:\n",
    "                logging.debug(f'quote was too long: {my_quote[\"len\"]} characters')\n",
    "        if my_quote['attribution']:\n",
    "            attribution = my_quote['attribution']\n",
    "            my_quote['attribution'] = f'{constants.attribution_char}{attribution}'\n",
    "        logging.debug(my_quote)\n",
    "        \n",
    "        data = my_quote\n",
    "        data['time'] = _time_now()\n",
    "        data['tag_image'] = constants.tag_image\n",
    "        is_updated = True\n",
    "        priority = self.max_priority\n",
    "        \n",
    "    return (is_updated, data, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.root.setLevel('DEBUG')\n",
    "\n",
    "# # use this for testing\n",
    "# from library.SelfDummy import SelfDummy\n",
    "# from library.CacheFiles import CacheFiles\n",
    "# from epdlib import Layout\n",
    "# self = SelfDummy()\n",
    "# self.max_priority = 1\n",
    "# self.cache = CacheFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_function(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook reddit_quote.ipynb to python\n",
      "[NbConvertApp] Writing 7589 bytes to reddit_quote.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter-nbconvert --to python --template python_clean reddit_quote.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook magic to create a symbolic to the library directory\n",
    "# this will allow you to import the Plugin() module \n",
    "!ln -s ../../library ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from library.CacheFiles import CacheFiles\n",
    "# def test_plugin():\n",
    "#     '''This code snip is useful for testing a plugin from within Jupyter Notebook'''\n",
    "#     from library import Plugin\n",
    "#     from IPython.display import display\n",
    "#     # this is set by PaperPi based on the configured schreen\n",
    "#     test_plugin = Plugin(resolution=(264, 176))\n",
    "#     # this is pulled from the configuration file; the appropriate section is passed\n",
    "#     # to this plugin by PaperPi during initial configuration\n",
    "#     test_plugin.config = {'max_length': '88'}\n",
    "#     test_plugin.layout = layout.quote_small_screen\n",
    "    \n",
    "#     # this is done automatically by PaperPi when loading the plugin\n",
    "#     test_plugin.cache = CacheFiles()\n",
    "#     test_plugin.update_function = update_function\n",
    "#     test_plugin.update()\n",
    "#     display(test_plugin.image)\n",
    "#     return test_plugin\n",
    "# my_plugin = test_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this simulates calling the plugin from PaperPi\n",
    "# d = my_plugin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epd_display-ApAYs8Kw",
   "language": "python",
   "name": "epd_display-apays8kw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
